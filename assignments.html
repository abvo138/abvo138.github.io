<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>About - My Website</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

    <header>
        <h1>Assignments</h1>
        <nav>
            <a href="index.html">About Me</a>
            <a href="work-experience.html">Work Experience</a>
            <a href="projects.html">Projects</a>
            <a href="contact.html">Contact</a>
            <a href="assignments.html">Assignments</a>

            <!--
            <a href="abvo138.github.io/">About Me</a>
            <a href="abvo138.github.io/work-experience">Work Experience</a>
            <a href="abvo138.github.io/projects">Projects</a>
            <a href="abvo138.github.io/contact">Contact</a>
            <a href="abvo138.github.io/assignments">Assignments</a>
            -->
        </nav>
    </header>

    <main>
        <h2>Assignments</h2>

        <!-- The name of the github repository for each assignment should be of the 
        format: bu_username-assignment-X (where X is the assignment number and bu_username 
        is your BU Email ID without @bu.edu), for example: given a bu email of ayush25@bu.edu 
        and working on assignment 1: “ayush25-assignment-1” will be the correct GitHub repo 
        name for assignment 1.
        -->

            <h3>Assignment 0</h3> <p>
            <h4><i> September 15</i></h4>
            <a href="https://github.com/abvo138/abvo138-assignment-0">Github Link</a></p>
            <p> Approach: Wrote a python script that takes in two numbers (num1 and num2), adds them, and prints their sum. 
                Tested with num1 = 1 and num2 = 2 - function printed 3 as expected. 

            </p>



            <h3>Assignment 1</h3> <p>
            <h4><i> September 22</i></h4>
            <a href="https://github.com/abvo138/abvo138-assignment-1">Github Link</a></p>
            <p> Approach: This assignment demonstrated how data can be used to find 
                insights even without advanced data science tools. Data was collected on elevator arrivals in CDS to see if an optimal position to stand and wait could 
                (beyond being in the middle) could be ascertained. I focused on ensuring good data manipulation by doing well 
                throughout groupings and consistent "sanity checks." I prioritized this because I knew that if something in 
                the data was funny, I would likely be unable to find a good optimal position. However, I was able to find a position 
                that led to a decrease in average walking compared to the naive position.
            </p>

            <h3>Assignment 2</h3> <p></p>
                <h4><i> September 29</i></h4>
                <a href="https://github.com/abvo138/abvo138-assignment-2">Github Link</a></p>
                <p> Approach: This assignment implements a manual version of KMeans that does not make use of any existing KMeans
                    libraries. It allows for four initialization options: Random, Farthest First, Kmeans++, and Manual (via point-and-click). 
                    The starting centroids of the algorithm are based on these initialization methods. After the user 
                    selects their desired number of clusters and initialization method, they can generate data and either 
                    step through KMeans or converge automatically. They can then reset the algorithm without changing 
                    the data and try a different number of clusters or a new initialization method. 
                </p>
                <p></p>
                <p>Demo Video:</p>
                <!-- Embed a YouTube video -->
                <iframe width="560" height="315" 
                src="https://www.youtube.com/embed/YTUiussz-Ts?si=hvMYwZNkbYF5zGbk" 
                title="YouTube video player" frameborder="0" 
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
                referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            
            <h3>Assignment 3</h3> <p>
                <h4><i> October 6</i></h4>
                <a href="https://github.com/abvo138/abvo138-assignment-3">Github Link</a></p>
                <p> Approach: This assigment implements a manual version of Singular Value Decomposition (SVD) and then 
                    trains a logistic regression using the various forms of compressed data. SVD is used for compression, 
                    keeping only the most important data and reducing the number of features by taking only the largest singluar 
                    values from the data. In the analysis portion, I compared how the model performed using 5, 10, 50, 100, and 
                    200 SVD components. From the visualizations it was clear there was a tradeoff between accuracy and runtime. 
                    However, around 50 SVD components, the accuracy plateaus while the runtime continues to increase in an 
                    almost linear fashion. This highlights that the runtime cost of choosing more SVD components does not pay 
                    for the increase in accuracy.
                </p>

            <h3>Assignment 4</h3> <p></p>
            <h4><i> October 13</i></h4>
            <a href="https://github.com/abvo138/abvo138-assignment-4">Github Link</a></p>
            <p> Approach: This assignment implements a manual version of LSA and SVD, taking in search words and 
                returning documents with similar topics to the search. Uses Cosine Similarity to score how well
                documents match search word topics. 
            </p>
            <p></p>
            <p>Demo Video:</p>
            <!-- Embed a YouTube video -->
            <iframe width="560" height="315" 
            src="https://www.youtube.com/embed/fu8O5JuIoPg?si=h6jzO0lRCdWqhvWV" 
            title="YouTube video player" frameborder="0" 
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
            referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

            <h3>Assignment 5</h3> <p></p>
                <h4><i> October 20</i></h4>
                <a href="https://github.com/abvo138/abvo138-assignment-5">Github Link</a></p>
                <p> Approach: This assignment was a Kaggle competition that involved predicting bank churn. To clean the 
                    data, I manually one hot encoded the categorical variables, scaled the numerical variables, and dropped
                    outlier variables using z-score. The model used a manual implementation of KNN and KFold cross-validation.  
                    It was then evaluated using a manual ROC score. On the test data, the best-trained model received a 
                    score of 0.89. 
                </p>

            <h3>Midterm</h3> <p></p>
                <h4><i> October 28</i></h4>
                <a href="https://github.com/abvo138/abvo138-CS506-midterm">Github Link</a></p>
                <p> Approach: Midterm Kaggle competition trying to predict product scores based on previous reviews and scores.
                </p>

            <h3>Assignment 6</h3> <p></p>
                <h4><i> November 3</i></h4>
                <a href="https://github.com/abvo138/abvo138-assignment-6">Github Link</a></p>
                <p> Approach: This assignment created an app that visualizes how changing a dataset's sample size (N), mean (mu), and variance 
                    (sigma^2) affects linear regression. This app asks for the aforementioned inputs as well as for the number of simulations it should 
                    run and then displays both a scatterplot of the dataset fitted with linear regression and a histogram of the slope and 
                    intercept values from all of the simulations. Additional information included with these graphs are the slope and the 
                    intercept of the linear regression and the proportions of slopes and intercepts from the simulations that were more extreme than 
                    those shown in the first graph.  
                </p>
                <p></p>
            <p>Demo Video:</p>
            <!-- Embed a YouTube video -->
            <iframe width="560" height="315" src="https://www.youtube.com/embed/SWPmBfBKUbw?si=VkI-oVbJoIDX5SiC" 
            title="YouTube video player" frameborder="0" 
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
            referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            
            <h3>Assignment 7</h3> <p></p>
                <h4><i> November 10</i></h4>
                <a href="https://github.com/abvo138/abvo138-assignment-7">Github Link</a></p>
                <p> Approach: This assignment built upon Assignment 6 by adding hypothesis testing and confidence interval calculations for testing 
                    parameters such as the slope and intercept. Hypothesis testing included three types of tests: 'not equal to,' 'greater than,' 
                    and 'less than,' with corresponding p-values to assess evidence against the null hypothesis. Confidence intervals were able to be 
                    calculated at 90%, 95%, and 99% levels, allowing for an assessment of whether the true parameter values fell within these intervals 
                    as well as the mean estimates.
                </p>
                <p></p>
            <p>Demo Video:</p>
            <!-- Embed a YouTube video -->
            <iframe width="560" height="315" src="https://www.youtube.com/embed/rp2BokhgM9Q?si=_mpJAp-ySlw-EhLz" 
            title="YouTube video player" frameborder="0" 
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
            referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            
            <h3>Assignment 8</h3> <p></p>
                <h4><i> November 17</i></h4>
                <a href="https://github.com/abvo138/abvo138-assignment-8">Github Link</a></p>
                <p> Approach: This assignment visualized logistic regression using two clusters. One of these clusters was fixed and the other was shifted 
                    incrementally on y=-x. Doing this shifting helped show how the parameters of logistic regression - intercept (β0), coefficients (β1​ and β2​), 
                    slope (−β1/β2), intercept ratio (β0/β2​), logistic loss, and margin width - also change based on the position of the clusters. The webpage
                    includes visualizations of all these parameters in every shift distance. 
                </p>
                <p></p>
            <p>Demo Video:</p>
            <!-- Embed a YouTube video -->
            <iframe width="560" height="315" src="https://www.youtube.com/embed/ohFIwHUEIf8?si=9w_1_CLS1irJE8Tw" 
            title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
            referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

            So here we are generating two clusters and fitting logistic regressions. 

    </main>


    <footer>
        <p>&copy; 2024 - Anneke van Oosterom</p>
    </footer>

</body>
</html>
