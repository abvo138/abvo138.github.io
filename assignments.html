<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>About - My Website</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

    <header>
        <h1>Assignments</h1>
        <nav>
            <a href="index.html">About Me</a>
            <a href="work-experience.html">Work Experience</a>
            <a href="projects.html">Projects</a>
            <a href="contact.html">Contact</a>
            <a href="assignments.html">Assignments</a>

            <!--
            <a href="abvo138.github.io/">About Me</a>
            <a href="abvo138.github.io/work-experience">Work Experience</a>
            <a href="abvo138.github.io/projects">Projects</a>
            <a href="abvo138.github.io/contact">Contact</a>
            <a href="abvo138.github.io/assignments">Assignments</a>
            -->
        </nav>
    </header>

    <main>
        <h2>Assignments</h2>

        <!-- The name of the github repository for each assignment should be of the 
        format: bu_username-assignment-X (where X is the assignment number and bu_username 
        is your BU Email ID without @bu.edu), for example: given a bu email of ayush25@bu.edu 
        and working on assignment 1: “ayush25-assignment-1” will be the correct GitHub repo 
        name for assignment 1.
        -->

            <h3>Assignment 0</h3> <p>
            <h4><i> September 15</i></h4>
            <a href="https://github.com/abvo138/abvo138-assignment-0">Github Link</a></p>
            <p> Approach: Wrote a python script that takes in two numbers (num1 and num2), adds them, and prints their sum. 
                Tested with num1 = 1 and num2 = 2 - function printed 3 as expected. 

            </p>



            <h3>Assignment 1</h3> <p>
            <h4><i> September 22</i></h4>
            <a href="https://github.com/abvo138/abvo138-assignment-1">Github Link</a></p>
            <p> Approach: This assignment demonstrated how data can be used to find 
                insights even without advanced data science tools. Data was collected on elevator arrivals in CDS to see if an optimal position to stand and wait could 
                (beyond being in the middle) could be ascertained. I focused on ensuring good data manipulation by doing well 
                throughout groupings and consistent "sanity checks." I prioritized this because I knew that if something in 
                the data was funny, I would likely be unable to find a good optimal position. However, I was able to find a position 
                that led to a decrease in average walking compared to the naive position.
            </p>

            <h3>Assignment 2</h3> <p></p>
                <h4><i> September 29</i></h4>
                <a href="https://github.com/abvo138/abvo138-assignment-2">Github Link</a></p>
                <p> Approach: This assignment implements a manual version of KMeans that does not make use of any existing KMeans
                    libraries. It allows for four initialization options: Random, Farthest First, Kmeans++, and Manual (via point-and-click). 
                    The starting centroids of the algorithm are based on these initialization methods. After the user 
                    selects their desired number of clusters and initialization method, they can generate data and either 
                    step through KMeans or converge automatically. They can then reset the algorithm without changing 
                    the data and try a different number of clusters or a new initialization method. 
                </p>
                <p></p>
                <p>Demo Video:</p>
                <!-- Embed a YouTube video -->
                <iframe width="560" height="315" 
                src="https://www.youtube.com/embed/YTUiussz-Ts?si=hvMYwZNkbYF5zGbk" 
                title="YouTube video player" frameborder="0" 
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
                referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            
            <h3>Assignment 3</h3> <p>
                <h4><i> October 6</i></h4>
                <a href="https://github.com/abvo138/abvo138-assignment-3">Github Link</a></p>
                <p> Approach: This assigment implements a manual version of Singular Value Decomposition (SVD) and then 
                    trains a logistic regression using the various forms of compressed data. SVD is used for compression, 
                    keeping only the most important data and reducing the number of features by taking only the largest singluar 
                    values from the data. In the analysis portion, I compared how the model performed using 5, 10, 50, 100, and 
                    200 SVD components. From the visualizations it was clear there was a tradeoff between accuracy and runtime. 
                    However, around 50 SVD components, the accuracy plateaus while the runtime continues to increase in an 
                    almost linear fashion. This highlights that the runtime cost of choosing more SVD components does not pay 
                    for the increase in accuracy.
                </p>

            <h3>Assignment 4</h3> <p></p>
            <h4><i> October 13</i></h4>
            <a href="https://github.com/abvo138/abvo138-assignment-4">Github Link</a></p>
            <p> Approach: This assignment implements a manual version of LSA and SVD, taking in search words and 
                returning documents with similar topics to the search. Uses Cosine Similarity to score how well
                documents match search word topics. 
            </p>
            <p></p>
            <p>Demo Video:</p>
            <!-- Embed a YouTube video -->
            <iframe width="560" height="315" 
            src="https://www.youtube.com/embed/fu8O5JuIoPg?si=h6jzO0lRCdWqhvWV" 
            title="YouTube video player" frameborder="0" 
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
            referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

            <h3>Assignment 5</h3> <p></p>
                <h4><i> October 20</i></h4>
                <a href="https://github.com/abvo138/abvo138-assignment-5">Github Link</a></p>
                <p> Approach: This assignment was a Kaggle competition that involved predicting bank churn. To clean the 
                    data, I manually one hot encoded the categorical variables, scaled the numerical variables, and dropped
                    outlier variables using z-score. The model used a manual implementation of KNN and KFold cross-validation.  
                    It was then evaluated using a manual ROC score. On the test data, the best-trained model received a 
                    score of 0.89. 
                </p>

            <h3>Midterm</h3> <p></p>
                <h4><i> October 28</i></h4>
                <a href="https://github.com/abvo138/abvo138-CS506-midterm">Github Link</a></p>
                <p> Approach: Midterm Kaggle competition trying to predict product scores based on previous reviews and scores.
                </p>
                
    </main>

    <footer>
        <p>&copy; 2024 - Anneke van Oosterom</p>
    </footer>

</body>
</html>
